from nltk.tokenize import word_tokenize, sent_tokenize

text = "I love NLP. It's amazing!"
# Word Tokenization
words = word_tokenize(text)
print("Word Tokenization:", words)

# Sentence Tokenization
sentences = sent_tokenize(text)
print("Sentence Tokenization:", sentences)
